{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f688372d-f233-4ddc-820e-e598b4277c04",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PySpark"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de9588f7-02ef-436b-8633-c0d057941505",
   "metadata": {},
   "source": [
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea6de5e-c0c9-49cb-b908-948d05e8520d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b986d8f-3494-410d-985d-682fd81991d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, asc, desc, lit, concat_ws, current_date\n",
    "from pyspark.sql.functions import approx_count_distinct,collect_list\n",
    "from pyspark.sql.functions import collect_set,sum,avg,max,countDistinct,count\n",
    "from pyspark.sql.functions import first, last, kurtosis, min, mean, skewness \n",
    "from pyspark.sql.functions import stddev, stddev_samp, stddev_pop, sumDistinct\n",
    "from pyspark.sql.functions import variance,var_samp,  var_pop\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cec1051-d882-4986-b51d-b40baf7c800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize spark object\n",
    "spark = SparkSession.builder.master('local[2]').appName('my_app').getOrCreate()\n",
    "\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
    "  ]\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2ffe9c-b457-4970-a444-28e9e60fe7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe \n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "\n",
    "df = spark.read.csv(file_path)\n",
    "df = spark.read.options(header='true', inferSchema='true') \\\n",
    "          .csv(csv_filePath)\n",
    "\n",
    "# to and from pandas\n",
    "sparkDF2 = spark.createDataFrame(pandasDF,schema=mySchema)\n",
    "pandasDF2 = sparkDF2.select(\"*\").toPandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abccab5-abbd-4f5d-9d65-d3c079296f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining schema, structtypes\n",
    "schema = StructType([\n",
    "            StructField(\"city\", StringType(), True),\n",
    "            StructField(\"dates\", StringType(), True),\n",
    "            StructField(\"population\", IntegerType(), True)])\n",
    "structureSchema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('id', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('salary', IntegerType(), True)\n",
    "         ])\n",
    "\n",
    "data = [\n",
    "    (\"James Smith\", [\"Java\",\"Scala\",\"C++\"]), \n",
    "    (\"Michael,Rose,\", [\"Spark\",\"Java\",\"C++\"])\n",
    "]\n",
    "schema = StructType([\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('languages', ArrayType(StringType()), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ffa97c-735c-4338-9953-05f4c94431d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the dataframe\n",
    "df.shape()\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "df.limit(3)  #Returns a new Dataset by taking the first n rows.\n",
    "df.take(2)  #Action, Return Array[T]  # Internally calls limit and collect\n",
    "df.head(2)  #Return Array[T]\n",
    "df.tail(2)  #Return Array[T]\n",
    "df.collect()  # Return Array[T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68db9300-a545-4a26-8167-842eb1491cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinct\n",
    "distinctDF = df.distinct()\n",
    "n = df.count()\n",
    "d = distinctDF.count()\n",
    "dropDisDF = df.dropDuplicates([\"department\",\"salary\"])\n",
    "\n",
    "df.distinct()\n",
    "df.count()\n",
    "df.distinct().count()\n",
    "df.select(countDistinct(\"Dept\",\"Salary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3274434b-7942-495c-85df-a0aaa8ceb96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select\n",
    "df.select(df.gender)\n",
    "df.select(df[\"gender\"])\n",
    "df.select(col(\"gender\"))\n",
    "df.select(df.property.hair) #if property column is having array of [hair, eyes, legs]\n",
    "df.select(\"name.*\")\n",
    "df.select(\"name.firstname\", \"name.lastname\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeb8724-89ef-41b3-b386-d7efd95afc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to doubleType\n",
    "df.withColumn(\"salary\",df.salary.cast('double')).printSchema()    \n",
    "df.withColumn(\"salary\",df.salary.cast(DoublerType())).printSchema()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982da43e-90d0-4c14-9729-373c2de5284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the dataframe\n",
    "df.sort(\"department\",\"state\").show(truncate=False)\n",
    "df.sort(col(\"department\"),col(\"state\")).show(truncate=False)\n",
    "\n",
    "df.orderBy(\"department\",\"state\").show(truncate=False)\n",
    "df.orderBy(col(\"department\"),col(\"state\")).show(truncate=False)\n",
    "\n",
    "df.sort(df.department.asc(),df.state.asc()).show(truncate=False)\n",
    "df.sort(col(\"department\").asc(),col(\"state\").asc()).show(truncate=False)\n",
    "df.orderBy(col(\"department\").asc(),col(\"state\").asc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475335d4-4ffe-48b9-804a-7b1329b2b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the sql like query\n",
    "df.createOrReplaceTempView(\"EMP_VIEW\")\n",
    "df.createOrReplaceGlobalView(\"EMP_VIEW\")\n",
    "\n",
    "df.select(\"employee_name\",asc(\"department\"),desc(\"state\"),\"salary\",\"age\",\"bonus\")\n",
    "spark.sql(\"select employee_name,department,state,salary,age,bonus \\\n",
    "          from EMP ORDER BY department asc\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3066c49-856d-4a38-9ddd-0565fed63363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#column operators\n",
    "df.select(df.col1 + df.col2).show()\n",
    "df.select(df.col1 - df.col2).show() \n",
    "df.select(df.col1 * df.col2).show()\n",
    "df.select(df.col1 / df.col2).show()\n",
    "df.select(df.col1 % df.col2).show()\n",
    "df.select(df.col2 > df.col3).show()\n",
    "df.select(df.col2 < df.col3).show()\n",
    "df.select(df.col2 == df.col3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b47ef8-b671-402f-b3df-6f619b37463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new column\n",
    "df.withColumn(\"bonus_percent\", lit(0.3))\n",
    "df.withColumn(\"bonus_amount\", df.salary*0.3)\n",
    "df.withColumn(\"name\", concat_ws(\",\",\"firstname\",'lastname'))\n",
    "df.withColumn(\"current_date\", current_date())\n",
    "df.select(\"firstname\",\"salary\", lit(0.3).alias(\"bonus\"))\n",
    "df.select(\"firstname\",\"salary\", lit(df.salary * 0.3))\n",
    "spark.sql(\"select firstname,salary, '0.3' as bonus from EMP_VIEW\")\n",
    "df.withColumn(\"grade\", \\\n",
    "   when((df.salary < 4000), lit(\"A\")) \\\n",
    "     .when((df.salary >= 4000) & (df.salary <= 5000), lit(\"B\")) \\\n",
    "     .otherwise(lit(\"C\")) \\\n",
    "  )\n",
    "\n",
    "df2 = df.withColumn(\"new_gender\", when(df.gender == \"M\",\"Male\") \\\n",
    "                                 .when(df.gender == \"F\",\"Female\") \\\n",
    "                                 .when(df.gender.isNull() ,\"\") \\\n",
    "                                 .otherwise(df.gender))\n",
    "\n",
    "# using getItem() to access elements of array\n",
    "df.withColumn(\"hair\", df.properties.getItem(\"hair\")) \\\n",
    "  .withColumn(\"eyes\", df.properties.getItem(\"eyes\")) \\\n",
    "  .drop(\"properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e29a42e-55d9-4af9-9ba2-198bdb9a7c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop column\n",
    "df.drop(df.firstname)\n",
    "df.drop('id', 'gender', 'age')\n",
    "\n",
    "## drop null records\n",
    "df.na.drop()\n",
    "df.na.drop(how=\"any\")\n",
    "df.na.drop(subset=[\"population\",\"type\"])\n",
    "df.dropna()\n",
    "\n",
    "#handle null values\n",
    "df.fillna(value=0)\n",
    "df.fillna(value=0,subset=[\"population\"])\n",
    "df.na.fill(value=0)\n",
    "df.na.fill(value=0,subset=[\"population\"])\n",
    "df.fillna({\"city\": \"unknown\", \"type\": \"Old\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3c94f4-c4f7-4372-8ade-f4745d7d289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd.map\n",
    "StructField('properties', MapType(StringType(),StringType()),True)\n",
    "df3=df.rdd.map(\n",
    "    lambda x: (x.name, x.properties[\"hair\"], x.properties[\"eye\"])\n",
    ").toDF([\"name\",\"hair\",\"eye\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccfbe77-dcd1-4267-a828-34b018e76041",
   "metadata": {},
   "outputs": [],
   "source": [
    "## aggregation\n",
    "df.select(collect_list(\"salary\"))\n",
    "df.select(collect_set(\"salary\"))\n",
    "df.select(countDistinct(\"department\", \"salary\"))\n",
    "df.select(first(\"salary\"))\n",
    "df.select(last(\"salary\"))\n",
    "df.select(max(\"salary\"))\n",
    "df.select(min(\"salary\"))\n",
    "df.select(kurtosis(\"salary\"))\n",
    "df.select(mean(\"salary\"))\n",
    "df.select(median(\"salary\"))\n",
    "df.select(mode(\"salary\"))\n",
    "df.select(skewness(\"salary\"))\n",
    "df.select(sum(\"salary\"))\n",
    "df.select(stddev(\"salary\"))\n",
    "df.select(sumDistinct(\"salary\"))\n",
    "df.select(variance(\"salary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99268de4-9fab-4cbb-a4de-05e3bfd83296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast column\n",
    "df2 = df.withColumn(\"age\",col(\"age\").cast(StringType())) \\\n",
    "    .withColumn(\"isGraduated\",col(\"isGraduated\").cast(BooleanType())) \\\n",
    "    .withColumn(\"jobStartDate\",col(\"jobStartDate\").cast(DateType()))\n",
    "df4 = spark.sql(\"SELECT STRING(age),BOOLEAN(isGraduated),DATE(jobStartDate) from EMP_VIEW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c444892-91c7-4a60-baa3-d4c4a031cbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## collect\n",
    "dataCollect = deptDF.collect()\n",
    "dataCollect = deptDF.select(\"dept_name\").collect()\n",
    "for row in dataCollect:\n",
    "    print(row['dep_name'] + ',' + row['dep_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227b7ced-16ff-4f48-a18e-66edcc10efcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter\n",
    "df.sort(df.fname.asc())\n",
    "df.select(df.fname,df.id.cast(\"int\"))\n",
    "df.filter(df.id.between(100,300))\n",
    "df.filter(df.fname.contains(\"Cruise\"))\n",
    "df.filter(df.fname.startswith(\"T\"))\n",
    "df.filter(col(\"name\").startswith(\"J\"))\n",
    "df.filter(df.fname.endswith(\"Cruise\"))\n",
    "df.filter(df.lname.isNull())\n",
    "df.filter(df.lname.isNotNull())\n",
    "df.filter(df.fname.like(\"%om\"))\n",
    "df.select(df.fname.substr(1,2).alias(\"substr\"))\n",
    "df.filter(df.id.isin(my_list))\n",
    "\n",
    "df.filter(df.state.isNull())\n",
    "df.filter(\"state IS NULL AND gender IS NULL\")\n",
    "df.filter(df.state.isNull() & df.gender.isNull())\n",
    "df.filter(df.state.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8269b649-9b11-4813-87e0-83f6dcd1eaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## groupby\n",
    "df.groupBy(\"department\").count()\n",
    "df.groupBy(\"department\",\"state\").sum(\"salary\",\"bonus\")\n",
    "df.groupBy(\"department\") \\\n",
    "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
    "         avg(\"salary\").alias(\"avg_salary\"), \\\n",
    "         sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
    "         max(\"bonus\").alias(\"max_bonus\")) \\\n",
    "    .where(col(\"sum_bonus\") >= 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dbe7d4-7cf5-4995-8141-132788c27b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## window function\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "df.withColumn(\"row_number\", row_number().over(windowSpec))\n",
    "df.withColumn(\"lag\",lag(\"salary\",2).over(windowSpec))\n",
    "# can use rank(), dense_rank(), ntile(), lag, lead\n",
    "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ef1f2a-57bc-46eb-9823-46f842b13ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when otherwise\n",
    "df2 = df.withColumn(\"new_gender\", when(col(\"gender\") == \"M\", \"Male\") \\\n",
    "                                 .when(col(\"gender\") == \"F\", \"Female\") \\\n",
    "                                 .otherwise(\"Unknown\")\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572a0ee3-975d-4810-88c3-a118949386d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expr: to use sql like expression within your pyspark code\n",
    "df3 = df.withColumn(\"new_gender\", expr(\"case when gender = 'M' then 'Male' \" + \n",
    "                       \"when gender = 'F' then 'Female' \" +\n",
    "                       \"else 'Unknown' end\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527819fb-7405-4f0f-9404-0386203b120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join dataframes\n",
    "empDF.join(addDF, empDF[\"emp_id\"] == addDF[\"emp_id\"])\n",
    "empDF.join(addDF,[\"emp_id\"]) \\\n",
    "     .join(deptDF,empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"])\n",
    "empDF.join(deptDF).where(empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"])\n",
    "\n",
    "df = df1.join(df2, (df1.A1 == df2.B1) & (df1.A2 == df2.B2))\n",
    "df = spark.sql(\"select * \\\n",
    "               from EMP e, DEPT d, ADD a \\\n",
    "               where e.emp_dept_id == d.dept_id and e.emp_id == a.emp_id\")\n",
    "\n",
    "# union\n",
    "unionDF = df.union(df2)\n",
    "unionDF = df.union(df2).distinct()\n",
    "unionAllDF = df.unionAll(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de15889e-a0ce-40ce-b73c-f4773b4358d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## datetime\n",
    "df.withColumn('date_type', col('observation_dt').cast('date'))\n",
    "df.withColumn('date_type', to_timestamp('observation_dt').cast('date'))\n",
    "df.withColumn(\"date_type\",to_date(\"input_timestamp\"))\n",
    "df.withColumn(\"ts\",to_timestamp(col(\"input_timestamp\")))\n",
    "df.withColumn('DiffInSeconds',col(\"end_timestamp\").cast(\"long\") - col('from_timestamp').cast(\"long\"))\n",
    "df.withColumn('end_timestamp', current_timestamp())\n",
    "\n",
    "df.select(col(\"input\"), to_date(col(\"input\"),\"MM-dd-yyyy\").alias(\"date\"))\n",
    "df.select(to_timestamp(lit('06-24-2019 12:01:19.000'),'MM-dd-yyyy HH:mm:ss.SSSS'))\n",
    "\n",
    "df2 = df.select(split(col(\"fullname\"), \",\").alias(\"NameArray\")).drop(\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f4d4db-1551-4e54-9c30-ac7b2396eca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## collects\n",
    "\n",
    "df.collect() returns Array of Row type.\n",
    "df.collect()[0] returns the first element in an array (1st row).\n",
    "df.collect[0][0] returns the value of the first row & first column.\n",
    "states = df.rdd.map(lambda x: x[3]).collect()\n",
    "\n",
    "# row\n",
    "collData=rdd.collect()\n",
    "print(collData)\n",
    "for row in collData:\n",
    "    print(row.name + \",\" +str(row.lang))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa9e55e-556c-4ae6-8409-510900ad1d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallelize()  -to create rdd from list collection\n",
    "sparkContext=spark.sparkContext\n",
    "rdd = sparkContext.parallelize([1,2,3,4,5])\n",
    "print(rdd.getNumPartitions(), rdd.first())\n",
    "\n",
    "emptyRDD = sparkContext.emptyRDD()\n",
    "emptyRDD = sparkContext.parallelize([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944ecb11-09b9-484f-832a-ecc45c9e996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RDD\n",
    "rdd = spark.sparkContext.textFile(\"/path/textFile.txt\")\n",
    "rdd = spark.sparkContext.emptyRDD\n",
    "rdd = spark.sparkContext.parallelize([],10) #This creates 10 partitions\n",
    "rdd = sparkContext.parallelize([1,2,3,4,56,7,8,9,12,3], 10)\n",
    "\n",
    "# repartition and coalesce\n",
    "reparRdd = rdd.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f1394d-cbf6-407e-b601-d7bc8cc1de1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations\n",
    "rdd = spark.sparkContext.textFile(\"/tmp/test.txt\")\n",
    "\n",
    "rdd2 = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "rdd3 = rdd2.map(lambda x: (x,1))\n",
    "rdd4 = rdd3.reduceByKey(lambda a,b: a+b)\n",
    "rdd5 = rdd4.map(lambda x: (x[1],x[0])).sortByKey()\n",
    "rdd4 = rdd3.filter(lambda x : 'an' in x[1])\n",
    "\n",
    "#actions\n",
    "rdd6.count()\n",
    "rdd.first()\n",
    "rdd.max()\n",
    "rdd6.reduce(lambda a, b: (a[0]+b[0],a[1]))\n",
    "data3 = rdd6.take(3)  #  Returns the record specified as an argument.\n",
    "data = rdd6.collect() # Returns all data from RDD as an array.\n",
    "rdd6.saveAsTextFile(\"/tmp/wordCount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfb34ae-93e1-4792-aeb1-6f2dbf044dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache and persist\n",
    "\n",
    "cachedRdd = rdd.cache()  # PySpark cache() method in RDD class internally calls persist() \n",
    "dfPersist = rdd.persist(pyspark.StorageLevel.MEMORY_ONLY)\n",
    "rddPersist2 = rddPersist.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b72666-3686-4f32-877e-b8dc687ee8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversions\n",
    "dfFromRDD1 = rdd.toDF()  # Converts RDD to DataFrame\n",
    "rdd = df.rdd  # Convert DataFrame to RDD\n",
    "\n",
    "dfFromRDD2 = rdd.toDF(\"col1\",\"col2\")\n",
    "df = spark.createDataFrame(rdd).toDF(\"col1\",\"col2\")\n",
    "\n",
    "# list to df\n",
    "dep = [Row(\"Finance\",10), Row(\"Marketing\",20), Row(\"Sales\",30)]\n",
    "rdd = spark.sparkContext.parallelize(dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c6f7d8-4607-47b4-82ca-8f294da43b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and write csv\n",
    "schema = StructType() \\\n",
    "      .add(\"RecordNumber\",IntegerType(),True) \\\n",
    "      .add(\"Zipcode\",IntegerType(),True) \\\n",
    "      .add(\"ZipCodeType\",StringType(),True)\n",
    "df = spark.read.options(header='True', delimiter=',').csv(\"C:/resources/zipcodes.csv\")\n",
    "df.write.option(\"header\", True).csv(\"/tmp/zipcodes123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af8d245-4b42-45a9-859c-c83672a53a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming column\n",
    "df.withColumnRenamed(\"dob\",\"DateOfBirth\") \\\n",
    "  .withColumnRenamed(\"salary\",\"salary_amount\")\n",
    "\n",
    "schema2 = StructType(.....)\n",
    "df.select(col(\"name\").cast(schema2))\n",
    "\n",
    "df.select(col(\"name.firstname\").alias(\"fname\"),\n",
    "          col(\"name.middlename\").alias(\"mname\"),\n",
    "          col('salary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2bdb88-4504-4f05-98f4-76ff9a24cfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition - performs full shuffle, costly operation\n",
    "newDF=df.repartition(3)\n",
    "df2=df.repartition(3, \"state\")\n",
    "\n",
    "newDF.write.option(\"header\",True).mode(\"overwrite\").csv(\"/tmp/zipcodes-state\")\n",
    "\n",
    "df = spark.range(0,20)\n",
    "print(df.rdd.getNumPartitions())\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"500\")\n",
    "\n",
    "\n",
    "# SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('app.com') \\\n",
    "                    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "name = sc.appName\n",
    "master = sc.master\n",
    "sparkSession3 = SparkSession.newSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e20f6f-ed49-4291-89d8-752efb7c7455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a987650-bcae-4238-88a4-1a5f8550d871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0521dd3d-8cf8-4196-b0db-d00f81ce2e76",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7685feb-ac5f-40ac-a49e-01dfb58edc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark:SparkSession = SparkSession.builder() \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"SparkByExamples.com\") \\\n",
    "      .getOrCreate()\n",
    "      \n",
    "\"\"\"\n",
    "master() – If you are running it on the cluster you need to use your master name as an argument to master(). usually, it would be either yarn (Yet Another Resource Negotiator) or mesos depends on your cluster setup.\n",
    "\n",
    "Use local[x] when running in Standalone mode. x should be an integer value and should be greater than 0; this represents how many partitions it should create when using RDD, DataFrame, and Dataset. Ideally, x value should be the number of CPU cores you have.\n",
    "appName() – Used to set your application name.\n",
    "\n",
    "getOrCreate() – This returns a SparkSession object if already exists, and creates a new one if not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61da1095-5e0f-4fb2-91e4-a3ab6f8823f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallelize is a function in SparkContext and is used to create an RDD from a list collection.\n",
    "RDD, Resilient Distributed Datasets (RDD) is a fundamental data structure of PySpark, It is an immutable distributed collection of objects. Each dataset in RDD is divided into logical partitions, which may be computed on different nodes of the cluster.\n",
    "Partitions are basic units of parallelism in PySpark. RDDs in PySpark are a collection of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f93b52-755f-4408-9c16-943def87726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PySpark RDD/DataFrame collect() is an action operation that is used to retrieve all the elements of the dataset (from all nodes) as an Array of row type to the driver node. We should use the collect() on smaller dataset usually after filter(), group() e.t.c. Retrieving larger datasets results in OutOfMemory error.\n",
    "Note that collect() is an action hence it does not return a DataFrame instead, it returns data in an Array to the driver. Once the data is in an array, you can use python for loop to process it further.\n",
    "df.select() is a transformatoin whereas df.collect() is an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88b74aa-7592-443f-a7f2-0349cc846d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD (Resilient Distributed Dataset) is a fundamental building block of PySpark which is fault-tolerant, immutable distributed collections of objects. Immutable meaning once you create an RDD you cannot change it. Each record in RDD is divided into logical partitions, which can be computed on different nodes of the cluster. \n",
    "RDDs are a collection of objects similar to list in Python, with the difference being RDD is computed on several processes scattered across multiple physical servers also called nodes in a cluster while a Python collection lives and process in just one process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6adccb3-1075-4a32-86af-8b65f52ec772",
   "metadata": {},
   "outputs": [],
   "source": [
    "PySpark loads the data from disk and process in memory and keeps the data in memory, this is the main difference between PySpark and Mapreduce (I/O intensive). In between the transformations, we can also cache/persists the RDD in memory to reuse the previous computations.\n",
    "\n",
    "IMMUTABLE: once RDDs are created you cannot modify. When we apply transformations on RDD, PySpark creates a new RDD and maintains the RDD Lineage.\n",
    "FAULT_TAULRENT: any RDD operation fails, it automatically reloads the data from other partitions. PySpark task failures are automatically recovered for a certain number of times.\n",
    "LAZY_EVALUATION: it keeps the all transformations as it encounters(DAG) and evaluates the all transformation when it sees the first RDD action.\n",
    "PARTITIONING:  by default partitions the elements in a RDD. By default it partitions to the number of cores available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ed2457-e816-40cb-afeb-b9b86e25b9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD’s are created primarily in two different ways,\n",
    "\n",
    "parallelizing an existing collection and\n",
    "referencing a dataset in an external storage system (HDFS, S3 and many more). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0163774b-5a02-4fe1-925b-5c5d44d5832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sometimes we may need to repartition the RDD, PySpark provides two ways to repartition; first using repartition() method which shuffles data from all nodes also called full shuffle and second coalesce() method which shuffle data from minimum nodes, for examples if you have data in 4 partitions and doing coalesce(2) moves data from just 2 nodes.  \n",
    "Both of the functions take the number of partitions to repartition rdd as shown below.  Note that repartition() method is a very expensive operation as it shuffles data from all nodes in a cluster. \n",
    "The repartition() method is used to increase or decrease the number of partitions of an RDD or dataframe in spark. This method performs a full shuffle of data across all the nodes. It creates partitions of more or less equal in size. This is a costly operation given that it involves data movement all over the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515b9747-fd21-4219-bcdd-09d31981caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD transformations – Transformations are lazy operations, instead of updating an RDD, these operations return another RDD. they don’t execute until you call an action on RDD.\n",
    "RDD actions – operations that trigger computation and return RDD values.\n",
    "\n",
    "list = [1,2,3,4,5,6,7,8]\n",
    "rdd will be=>  partition1:[1,2], partition1:[3,4], partition1:[5,6], partition4:[7,8]\n",
    "\n",
    "When we use parallelize() or textFile() or wholeTextFiles() methods of SparkContxt to initiate RDD, it automatically splits the data into partitions based on resource availability. when you run it on a laptop it would create partitions as the same number of cores available on your system.\n",
    "\n",
    "Transformations on PySpark RDD returns another RDD and transformations are lazy meaning they don’t execute until you call an action on RDD. Some transformations on RDD’s are flatMap(), map(), reduceByKey(), filter(), sortByKey() and return new RDD instead of updating the current.\n",
    "flatmap\n",
    "map\n",
    "reduceByKey\n",
    "filter\n",
    "sortByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e502e7-2eda-489e-83ac-68cf6983ed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://sparkbyexamples.com/pyspark-rdd/ \n",
    "Types of RDD\n",
    "    PairRDDFunctions or PairRDD – Pair RDD is a key-value pair This is mostly used RDD type, \n",
    "    ShuffledRDD – \n",
    "    DoubleRDD – \n",
    "    SequenceFileRDD – \n",
    "    HadoopRDD – \n",
    "    ParallelCollectionRDD – "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb32c132-42fe-4f9e-a5d7-523a6185d367",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RDD cache and persist\n",
    "PySpark Cache and Persist are optimization techniques to improve the performance of the RDD jobs that are iterative and interactive.\n",
    "Though PySpark provides computation 100 x times faster than traditional Map Reduce jobs, If you have not designed the jobs to reuse the repeating computations you will see degrade in performance. Hence, we need to look at the computations and use optimization techniques.\n",
    "\n",
    "Using cache() and persist() methods, PySpark provides an optimization mechanism to store the intermediate computation of an RDD so they can be reused in subsequent actions.\n",
    "When you persist or cache an RDD, each worker node stores it’s partitioned data in memory or disk and reuses them in other actions on that RDD. And Spark’s persisted data on nodes are fault-tolerant meaning if any partition is lost, it will automatically be recomputed using the original transformations that created it.\n",
    "\n",
    "Advantages of persisting rdd:\n",
    "Cost efficient – PySpark computations are very expensive hence reusing the computations are used to save cost.\n",
    "Time efficient – Reusing the repeated computations saves lots of time.\n",
    "Execution time – Saves execution time of the job which allows us to perform more jobs on the same cluster.\n",
    "\n",
    "PySpark cache() method in RDD class internally calls persist() method which in turn uses sparkSession.sharedState.cacheManager.cacheQuery to cache the result set of RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f75181e-81aa-45df-8e91-e62b04d79ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PySpark Shared Variables: Broadcast and Accumulators\n",
    "\n",
    "When PySpark executes transformation using map() or reduce() operations, It executes the transformations on a remote node by using the variables that are shipped with the tasks and these variables are not sent back to PySpark Driver hence there is no capability to reuse and sharing the variables across tasks. PySpark shared variables solve this problem using the below two techniques. PySpark provides two types of shared variables.\n",
    "    Broadcast variables (read-only shared variable)\n",
    "    Accumulator variables (updatable shared variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ce63ef-0ab6-4ea5-8199-fb8b67831bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Difference between RDD and DataFrame and Dataset\n",
    "\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9185ff18-cf55-451a-b651-064ea4cdb02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Difference between Pyspark and MapReduce\n",
    "\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677851c2-0d5a-4971-a4d9-0d05ba1d9321",
   "metadata": {},
   "outputs": [],
   "source": [
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d2f7ac-e87c-47bf-b937-0a6a398118db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eedbf7-ad13-4f68-9b5e-7fba5bcf50a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
